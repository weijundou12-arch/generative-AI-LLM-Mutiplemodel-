Week 4 知识点（面试满分答案版）
章节重点 1：部署全链路与架构（从单机脚本到线上服务）

面试考点 1：把大模型部署成服务，最小闭环是什么？
满分简答：模型加载（一次）→请求接收→输入校验→推理生成→返回结果→日志与监控→健康检查。线上要保证可用性与可观测性，否则“能跑”不等于“能用”。

面试考点 2：推理服务的两种典型形态？
满分简答：一种是本地进程/单机服务（FastAPI + transformers），另一种是专用推理引擎（vLLM/TensorRT-LLM/llama.cpp）前面挂 API 网关。后者吞吐更高、并发更稳、KV 管理更专业。

面试考点 3：prefill vs decode 是什么？为什么要区分？
满分简答：prefill 是把 prompt 一次性编码并计算注意力，偏算力吞吐；decode 是逐 token 生成，偏 KV cache 访存与延迟。优化策略不同：prefill 关注矩阵乘法效率，decode 关注 KV 管理、批处理与内存带宽。

面试考点 4：为什么“并发”会让服务变慢甚至崩？
满分简答：因为 GPU 推理是共享资源，并发会引起显存压力、上下文切换、排队延迟；如果每个请求都长上下文+大输出，会被 KV cache 撑爆。正确做法是限制最大 tokens、做队列/批处理、选择 vLLM 这类引擎。

章节重点 2：FastAPI 服务化基础（你要能手写）

面试考点 5：FastAPI 为什么适合做模型服务？
满分简答：FastAPI 基于 ASGI，支持高并发、异步 IO、Pydantic 校验、自动 OpenAPI 文档；非常适合做推理网关层（但重计算仍需后台线程/进程或推理引擎）。

面试考点 6：async def 就一定更快吗？
满分简答：不一定。async 只对 IO 友好；模型推理是重计算/阻塞，需要 to_thread、进程池或交给 vLLM/独立 worker，否则会阻塞事件循环导致所有请求卡住。

面试考点 7：服务启动时为什么要“只加载一次模型”？
满分简答：模型加载耗时且占显存，如果每个请求加载一次会极慢且炸显存。正确方式是在应用 startup/lifespan 中加载并缓存模型对象。

面试考点 8：健康检查（health/readiness）有什么区别？
满分简答：health 表示进程活着；readiness 表示模型已加载、GPU 就绪、可以接流量。K8s/反向代理会用 readiness 决定是否转发请求。

章节重点 3：接口设计（OpenAI-compatible/业务接口）

面试考点 9：为什么建议做 OpenAI-compatible 接口（/v1/chat/completions）？
满分简答：生态兼容性强（前端/SDK/工具链复用），更容易接入现有产品，也方便后续替换底层模型或引擎（transformers → vLLM）。

面试考点 10：请求里必须做哪些输入校验？
满分简答：至少校验：max_tokens 上限、prompt 长度、温度/采样参数范围、消息结构合法、禁止空输入、禁止超大 payload。否则容易 OOM、超时、或被恶意请求拖垮。

面试考点 11：为什么要支持 stream（流式输出）？
满分简答：流式能显著改善用户体感延迟（先看到再等完整），也更适合长输出。实现通常用 SSE（Server-Sent Events）或 WebSocket；OpenAI 风格多用 SSE。

章节重点 4：推理参数与“可控生成”

面试考点 12：temperature/top_p/top_k 分别控制什么？
满分简答：temperature 控制整体随机性（越大越发散）；top_p 是核采样保留累计概率 p 的候选；top_k 保留概率最高的 k 个候选。工程上常用 top_p + temperature 的组合。

面试考点 13：为什么要做 stop sequences？
满分简答：停止符能避免模型跑到不该输出的段落（例如提示模板、重复签名），也能控制格式边界，是生产场景输出可控性的关键工具。

章节重点 5：性能优化（面试高频 + 工作刚需）

面试考点 14：吞吐优化的优先级怎么排？
满分简答：先控制输入输出 token（最有效），再用 KV cache/批处理/并发队列，然后考虑推理引擎（vLLM/FlashAttention），最后才是量化/TensorRT 等更重的工程。

面试考点 15：为什么 vLLM 往往比纯 transformers 更适合服务？
满分简答：vLLM 专门解决 decode 阶段的 KV 管理与调度，支持连续批处理与 PagedAttention，能在高并发长上下文下保持更高吞吐和更稳定的内存占用。

面试考点 16：量化（int8/int4）部署的收益与代价？
满分简答：收益是显存下降、吞吐提升、成本更低；代价可能是质量下降、算子兼容性与工程复杂度上升。一般先做服务稳定与限额，再逐步量化。

章节重点 6：安全与治理（生产必须）

面试考点 17：你会怎么做接口鉴权？
满分简答：最小可用是 API Key（Header），生产可用 JWT/OAuth2 + 网关；并结合限流与审计日志。鉴权不仅防滥用，也方便计费与追责。

面试考点 18：为什么必须限流（rate limit）？
满分简答：LLM 推理昂贵且容易被“长上下文+长输出”拖垮。限流能防止单用户/恶意流量耗尽 GPU 资源，保证整体可用性。

章节重点 7：可观测性（Logs/Metrics/Tracing）

面试考点 19：上线推理服务最关键的监控指标有哪些？
满分简答：QPS、P50/P95 延迟、错误率、GPU 显存/利用率、平均输入/输出 tokens、排队时间、超时数。没有这些就无法定位瓶颈与容量规划。

面试考点 20：为什么需要 request_id 与结构化日志？
满分简答：方便跨组件追踪同一请求链路，快速定位问题；结构化日志便于检索聚合（ELK/Cloud Logging）。

章节重点 8：上线方式（Docker/反向代理/进程模型）

面试考点 21：Uvicorn 直接跑和 Gunicorn+UvicornWorkers 有什么区别？
满分简答：Uvicorn 单进程适合开发；生产常用 Gunicorn 管理多 worker、优雅重启、崩溃拉起。但对 GPU 模型服务，多 worker 可能重复占显存，通常是1 GPU 对应 1 worker或使用专用推理引擎。

面试考点 22：为什么要反向代理（Nginx/Caddy）？
满分简答：负责 TLS、压缩、连接管理、超时、限流、静态资源与路由；后端 FastAPI 专注业务与推理。

“院内辅助生成病历/随访记录”：
前端把 system + user 消息发到 /v1/chat/completions，后端加载本地模型（或没有模型时用 Dummy 回退），支持流式输出 SSE、鉴权、限流、健康检查、基础 metrics、日志 request_id。
