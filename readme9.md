Week 9 面试考点（满分简答版）
A. 微调到底在解决什么

面试考点 1：SFT 是什么？为什么需要它？
满分简答：SFT（Supervised Fine-Tuning）是用“指令-回答”数据监督训练，让模型更符合特定任务/行业表达风格。需要它是因为：纯 prompt/RAG 只能临时引导；SFT 能把稳定行为“写进权重”，提升一致性、格式遵循和领域措辞。

面试考点 2：什么时候用微调，什么时候用 RAG？
满分简答：

RAG：知识经常更新、需要可追溯引用、避免胡编（比如法规/年报）。

微调：要固化能力（格式、结构化输出、风格、工具调用范式、分类/抽取能力）。
最佳实践常是 RAG + 微调：微调学“怎么用证据回答”，RAG提供“证据本身”。

面试考点 3：微调的最大风险是什么？
满分简答：过拟合与灾难性遗忘（模型变“窄”）、数据污染（把错误或敏感信息学进去）、以及对齐偏移（更容易越权/胡答）。所以必须做：训练/验证分离、质量控制、审计与回滚。

B. SFT 数据：面试最爱追问（也是成败关键）

面试考点 4：SFT 数据最重要的 3 个原则？
满分简答：一致性（同类问题同风格同格式）、高信噪比（少废话、少脏数据）、覆盖关键边界（难例/反例/拒答/不确定性表达）。

面试考点 5：什么叫“模板 template”，为什么训练/推理必须一致？
满分简答：template 决定了“对话如何拼接成模型输入”（system/user/assistant 的特殊 token 与格式）。训练和推理 template 不一致会导致模型学到的行为无法正确触发，表现显著下降。LlamaFactory 明确要求 model 与 template 对应，推理也要用相同 template。

面试考点 6：怎么设计“行业数据集”（金融/医疗都通用）？
满分简答：用“任务族”覆盖能力：

信息抽取（数字/实体/时间）

结构化写作（摘要/报告/要点）

证据式回答（引用/不确定性）

负样本（证据不足→拒答/建议补充）

面试考点 7：数据泄漏怎么防？
满分简答：按文档/患者/公司维度切分（不要同一来源同时进 train 和 val），并对模板化样本去重；评测用“新文档/新分布”检验泛化。

C. LoRA：为什么它是工业默认

面试考点 8：LoRA 的核心思想是什么？
满分简答：不直接更新全量权重，而是在关键线性层旁路加一个低秩矩阵（A×B），只训练这部分参数，达到接近全参微调的效果但显存/存储/训练成本大幅下降。

面试考点 9：LoRA 常加在哪里？为什么？
满分简答：优先加在 Attention 的投影层（Q/K/V/O） 和/或 MLP 的线性层，因为这些层决定了信息路由与表达能力，少量参数就能显著改变行为。

面试考点 10：rank(r)、alpha、dropout 怎么理解？
满分简答：

r：容量（越大越能学，但越容易过拟合/更慢）

alpha：缩放系数（影响更新幅度）

dropout：正则（稳定训练，减过拟合）
调参策略：先小 r（8/16），不够再加。

D. QLoRA：低显存训练的关键

面试考点 11：QLoRA 和 LoRA 的区别？
满分简答：QLoRA 让“基座模型以 4bit 量化形式加载”，但仍训练 LoRA 适配器（通常 fp16/bf16），从而显著降低显存占用，单卡更容易跑大模型。

面试考点 12：QLoRA 为什么可能更慢/不稳？
满分简答：量化带来额外算子与精度噪声，可能导致吞吐下降或收敛更敏感；所以更依赖：学习率、梯度累积、checkpoint、合适的 quant method（如 bitsandbytes）。LlamaFactory 提供 quantization_bit / quantization_method 配置示例。

E. 训练稳定性：你得能讲“为什么这样配”

面试考点 13：为什么要 gradient_accumulation_steps？
满分简答：显存放不下大 batch 时，用梯度累积模拟大 batch，提高训练稳定性与收敛质量（但训练更慢）。

面试考点 14：bf16 vs fp16 怎么选？
满分简答：支持 bf16 的卡（A100/H100/多数新卡）优先 bf16：数值更稳、溢出更少；不支持则 fp16 配合 loss scaling。

面试考点 15：cutoff_len 是什么？为什么要管它？
满分简答：单条样本最大序列长度。太短学不到长上下文能力，太长显存爆炸且训练慢。工程上：先 1024 起步，按任务需要提升。

面试考点 16：学习率怎么定（LoRA/QLoRA）？
满分简答：LoRA 通常比全参略大（如 1e-4 量级起步），QLoRA 往往更敏感，需要更小或更长 warmup。LlamaFactory 的 SFT 示例中也用 1e-4、cosine、warmup_ratio 作为常见起点。

F. 评测与上线：面试“工程味”加分点

面试考点 17：SFT 评测怎么做才算专业？
满分简答：分层：

训练曲线（loss 是否正常、是否过拟合）

自动评测（任务指标：抽取 F1/分类 AUC/格式合规率）

人评或 LLM-as-judge（对齐与可读性）

线上监控（拒答率、幻觉率、失败率、成本）

面试考点 18：LoRA 训练完为什么要 merge？
满分简答：推理时不想每次加载“基座+adapter”两套，merge 能导出一个合并模型；LlamaFactory 提供 llamafactory-cli export 合并与导出配置。

面试考点 19：推理怎么做？
满分简答：可以直接用 LlamaFactory chat/webchat/api，或走 vLLM batch 推理。推理配置里要指定 model、template；若是 LoRA 模型还要指定 adapter 与 finetuning_type。
